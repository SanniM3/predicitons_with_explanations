final commands------deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7004-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7004     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:20:33,050] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:33,058] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:33,059] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:33,062] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:33,876] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:20:33,877] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29801 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-3639-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 3639 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:20:33,920] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:20:33,920] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29802 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-6290-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 6290 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:20:33,922] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:20:33,922] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29800 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-7004-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 7004 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:20:33,927] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:20:33,927] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbM119 --master_addr=127.0.0.1 --master_port=29803 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-9428-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 9428 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:20:36,543] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:36,576] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:36,683] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:36,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,173] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:20:37,173] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-05-29 19:20:37,173] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:20:37,173] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:20:37,173] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:20:37,173] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-05-29 19:20:37,174] [INFO] [launch.py:253:main] process 20285 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7004-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7004', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:20:37,298] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,298] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,298] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:20:37,298] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:20:37,298] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,299] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,299] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:20:37,299] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-05-29 19:20:37,299] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:20:37,299] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:20:37,299] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:20:37,299] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-05-29 19:20:37,300] [INFO] [launch.py:253:main] process 20286 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-3639-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '3639', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:20:37,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,415] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:20:37,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:20:37,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,416] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,416] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:20:37,416] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3]}
[2024-05-29 19:20:37,416] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:20:37,416] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:20:37,416] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:20:37,416] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3
[2024-05-29 19:20:37,417] [INFO] [launch.py:253:main] process 20287 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-9428-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '9428', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:20:37,420] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:20:37,420] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-05-29 19:20:37,420] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:20:37,420] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:20:37,420] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:20:37,420] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-05-29 19:20:37,421] [INFO] [launch.py:253:main] process 20288 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-6290-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '6290', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:20:43,869] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:43,966] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:44,064] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:20:44,064] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 19:20:44,080] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:44,165] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:20:44,165] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:20:44,165] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 19:20:44,271] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:20:44,271] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 19:20:44,357] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:20:44,358] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Total number of parameters 6738423808
Total number of parameters 6738423808
Total number of parameters 6738423808
[2024-05-29 19:31:14,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:31:14,732] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:31:14,741] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
Total number of parameters 6738423808
[2024-05-29 19:31:15,320] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:31:20,557] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:31:20,967] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:31:20,967] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:31:21,602] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.428830623626709 seconds
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.416456937789917 seconds
Time to load cpu_adam op: 2.4447901248931885 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:31:25,195] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:31:25,195] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:31:25,207] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:31:25,207] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:31:25,207] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:31:25,207] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:31:25,207] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:31:25,207] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:31:25,207] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:31:25,563] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:31:25,563] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:31:25,575] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:31:25,575] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:31:25,575] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:31:25,575] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:31:25,575] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:31:25,575] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:31:25,575] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:31:25,655] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:31:25,655] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
ninja: no work to do.
[2024-05-29 19:31:25,666] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:31:25,666] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:31:25,666] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:31:25,666] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:31:25,666] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:31:25,666] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:31:25,666] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Time to load cpu_adam op: 2.4038443565368652 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:31:26,160] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:31:26,160] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:31:26,171] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:31:26,171] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:31:26,171] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:31:26,171] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:31:26,171] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:31:26,171] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:31:26,171] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 19:32:47,381] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 20288
[2024-05-29 19:32:47,579] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-6290-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '6290', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 19:32:56,376] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 20286
[2024-05-29 19:32:56,377] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-3639-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '3639', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 19:35:35,384] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:35:36,280] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:35:54,211] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:35:54,211] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29801 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-4864-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 4864 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:35:54,223] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:35:54,223] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29802 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-7056-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 7056 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:35:56,837] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:35:56,856] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:35:57,476] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:35:57,476] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-05-29 19:35:57,476] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:35:57,476] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:35:57,476] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:35:57,476] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-05-29 19:35:57,477] [INFO] [launch.py:253:main] process 22089 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4864-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4864', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:35:57,543] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:35:57,543] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:35:57,544] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:35:57,544] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:35:57,544] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:35:57,544] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:35:57,544] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:35:57,544] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-05-29 19:35:57,544] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:35:57,544] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:35:57,544] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:35:57,544] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-05-29 19:35:57,545] [INFO] [launch.py:253:main] process 22090 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7056-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7056', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:36:04,662] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:36:04,679] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:36:04,843] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:36:04,843] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 19:36:04,867] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:36:04,868] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 19:42:25,982] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 20287
[2024-05-29 19:42:25,984] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-9428-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '9428', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 19:42:37,743] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-29 19:42:37,744] [INFO] [utils.py:801:see_memory_usage] MA 25.72 GB         Max_MA 25.72 GB         CA 25.73 GB         Max_CA 26 GB 
[2024-05-29 19:42:37,744] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 128.93 GB, percent = 14.4%
[2024-05-29 19:42:55,812] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:42:56,595] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:42:56,596] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbM119 --master_addr=127.0.0.1 --master_port=29803 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-4273-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 4273 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
Total number of parameters 6738423808
Total number of parameters 6738423808
[2024-05-29 19:42:58,983] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:42:59,201] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:42:59,763] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:43:00,530] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:43:00,530] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3]}
[2024-05-29 19:43:00,530] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:43:00,530] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:43:00,531] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:43:00,531] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3
[2024-05-29 19:43:00,531] [INFO] [launch.py:253:main] process 22906 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4273-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4273', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:43:05,097] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 20285
[2024-05-29 19:43:05,098] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7004-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7004', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 19:43:05,972] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:43:06,262] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:43:07,840] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:43:08,037] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:43:08,037] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 19:43:09,284] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 8.66659927368164 seconds
Time to load cpu_adam op: 8.513732433319092 seconds
[2024-05-29 19:43:22,286] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:43:22,286] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29800 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-7632-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 7632 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:43:24,882] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:43:24,882] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:43:24,985] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:43:24,985] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:43:24,986] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:43:24,986] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:43:24,986] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:43:24,986] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:43:24,986] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:43:30,876] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:43:30,877] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:43:31,179] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:43:31,180] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:43:31,180] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:43:31,276] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:43:31,276] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:43:31,276] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:43:31,276] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 19:44:21,786] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:44:38,980] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:44:38,980] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-05-29 19:44:38,980] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:44:38,980] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:44:38,980] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:44:38,980] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-05-29 19:44:39,180] [INFO] [launch.py:253:main] process 23456 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7632-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7632', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:46:25,591] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:46:30,479] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:46:30,479] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Total number of parameters 6738423808
[2024-05-29 19:49:01,077] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:51:49,281] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:52:31,380] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 22089
[2024-05-29 19:52:31,382] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4864-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4864', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 15.699439525604248 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:53:03,122] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:53:03,123] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:53:03,134] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:53:03,134] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:53:03,134] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:53:03,134] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:53:03,134] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:53:03,134] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:53:03,134] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 19:53:07,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:53:08,101] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:53:08,101] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29801 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-2689-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 2689 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
Total number of parameters 6738423808
[2024-05-29 19:53:10,575] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:53:10,907] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:53:11,194] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:53:11,194] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 19:53:11,194] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 19:53:11,195] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 19:53:11,195] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 19:53:11,195] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 19:53:11,195] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 19:53:11,195] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-05-29 19:53:11,195] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 19:53:11,195] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 19:53:11,195] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 19:53:11,195] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-05-29 19:53:11,196] [INFO] [launch.py:253:main] process 23983 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-2689-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '2689', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 19:53:16,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 19:53:17,950] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:53:18,135] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 19:53:18,135] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 11.31031060218811 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 19:53:38,180] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 19:53:38,180] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 19:53:38,482] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 19:53:38,482] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 19:53:38,482] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 19:53:38,483] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 19:53:38,483] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 19:53:38,483] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 19:53:38,483] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 19:57:38,383] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-29 19:57:38,384] [INFO] [utils.py:801:see_memory_usage] MA 25.72 GB         Max_MA 25.72 GB         CA 25.73 GB         Max_CA 26 GB 
[2024-05-29 19:57:38,385] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 160.8 GB, percent = 17.9%
[2024-05-29 19:58:05,677] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 22906
[2024-05-29 19:58:05,680] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4273-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4273', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 19:58:16,281] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 22090
[2024-05-29 19:58:16,288] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7056-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7056', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 19:58:28,074] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:58:28,074] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:58:43,881] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:58:43,881] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbM119 --master_addr=127.0.0.1 --master_port=29803 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-4523-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 4523 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 19:58:44,488] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 19:58:44,580] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29802 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-8219-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 8219 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
Total number of parameters 6738423808
[2024-05-29 19:59:31,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 19:59:50,478] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 19:59:56,776] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:00:02,585] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:00:02,586] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-05-29 20:00:02,586] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:00:02,586] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:00:02,586] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:00:02,586] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-05-29 20:00:02,776] [INFO] [launch.py:253:main] process 24636 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-8219-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '8219', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:00:08,672] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:00:08,672] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3]}
[2024-05-29 20:00:08,672] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:00:08,672] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:00:08,672] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:00:08,672] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3
[2024-05-29 20:00:08,673] [INFO] [launch.py:253:main] process 24701 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4523-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4523', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:00:12,729] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:00:15,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:00:15,269] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:00:15,450] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:00:15,450] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:00:15,459] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:00:15,459] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 4.247846841812134 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:00:42,076] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:00:42,076] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:00:42,884] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:00:42,884] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:00:42,884] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:00:42,885] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:00:42,885] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:00:42,885] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:00:42,885] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:06:41,438] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-29 20:06:41,440] [INFO] [utils.py:801:see_memory_usage] MA 25.72 GB         Max_MA 25.72 GB         CA 25.73 GB         Max_CA 26 GB 
[2024-05-29 20:06:41,440] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 153.51 GB, percent = 17.1%
[2024-05-29 20:07:13,979] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 23456
[2024-05-29 20:07:13,980] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7632-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7632', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
Total number of parameters 6738423808
Total number of parameters 6738423808
[2024-05-29 20:08:24,788] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:08:30,779] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:08:44,904] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:08:45,655] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:08:45,655] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29800 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-2175-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 2175 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:08:48,032] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:08:48,580] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:08:48,580] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-05-29 20:08:48,580] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:08:48,580] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:08:48,580] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:08:48,580] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-05-29 20:08:48,581] [INFO] [launch.py:253:main] process 25507 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-2175-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '2175', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:08:48,608] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:08:48,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.4373881816864014 seconds
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.4063937664031982 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:08:53,242] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:08:53,242] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:08:53,252] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:08:53,253] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:08:53,253] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:08:53,253] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:08:53,253] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:08:53,253] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:08:53,253] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:08:53,671] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:08:53,672] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:08:53,693] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:08:53,693] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:08:53,693] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:08:53,693] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:08:53,693] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:08:53,693] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:08:53,693] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:08:55,337] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:08:55,535] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:08:55,535] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:12:34,878] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 23983
[2024-05-29 20:12:34,881] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-2689-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '2689', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
Total number of parameters 6738423808
[2024-05-29 20:15:21,478] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:15:33,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:15:40,788] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 24636
[2024-05-29 20:15:40,876] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-8219-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '8219', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 20:15:52,283] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:15:52,380] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29801 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-7356-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 7356 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:16:05,323] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:16:05,863] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:16:05,924] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:16:05,924] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-05-29 20:16:05,924] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:16:05,924] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:16:05,924] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:16:05,924] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-05-29 20:16:05,925] [INFO] [launch.py:253:main] process 26126 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7356-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7356', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:16:06,625] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:16:06,625] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29802 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-8975-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 8975 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:16:07,951] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:16:09,418] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:16:09,990] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:16:09,991] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:16:09,991] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:16:09,991] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:16:09,991] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:16:09,991] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:16:09,991] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:16:09,991] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-05-29 20:16:09,991] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:16:09,991] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:16:09,991] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:16:09,991] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-05-29 20:16:09,992] [INFO] [launch.py:253:main] process 26349 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-8975-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '8975', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.4444379806518555 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:16:12,605] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:16:12,606] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:16:12,617] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:16:12,617] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:16:12,617] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:16:12,617] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:16:12,617] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:16:12,617] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:16:12,617] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:16:13,140] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:16:13,324] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:16:13,324] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:17:03,477] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:17:08,782] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:17:08,782] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:22:25,076] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 24701
[2024-05-29 20:22:25,078] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4523-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4523', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
Total number of parameters 6738423808
[2024-05-29 20:24:25,686] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:24:26,293] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Total number of parameters 6738423808
[2024-05-29 20:24:27,378] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:24:27,381] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbM119 --master_addr=127.0.0.1 --master_port=29803 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-51-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 51 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:24:27,553] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:24:29,883] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:24:30,465] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:24:30,465] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3]}
[2024-05-29 20:24:30,465] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:24:30,465] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:24:30,465] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:24:30,465] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3
[2024-05-29 20:24:30,466] [INFO] [launch.py:253:main] process 27053 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-51-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '51', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:24:32,333] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:24:33,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 2.422642946243286 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:24:37,059] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:24:37,060] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:24:37,070] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:24:37,070] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:24:37,070] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:24:37,070] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:24:37,070] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:24:37,070] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:24:37,070] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:24:37,137] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
[2024-05-29 20:24:37,320] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:24:37,320] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
ninja: no work to do.
Time to load cpu_adam op: 2.417276620864868 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:24:37,843] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:24:37,843] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:24:37,854] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:24:37,854] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:24:37,854] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:24:37,854] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:24:37,854] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:24:37,854] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:24:37,854] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:28:30,681] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 27053
[2024-05-29 20:28:30,684] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-51-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '51', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-9428-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 9428     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: starting process on GPU 3
-------using deepspeed---------Command isdeepspeed --master_port 29803 --include localhost:3 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-9428-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 9428     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4273-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4273     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: starting process on GPU 3
-------using deepspeed---------Command isdeepspeed --master_port 29803 --include localhost:3 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4273-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4273     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4523-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4523     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: starting process on GPU 3
-------using deepspeed---------Command isdeepspeed --master_port 29803 --include localhost:3 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4523-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4523     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-51-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 51     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: starting process on GPU 3
-------using deepspeed---------Command isdeepspeed --master_port 29803 --include localhost:3 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-51-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 51     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19741: finished
[2024-05-29 20:28:32,699] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 25507
[2024-05-29 20:28:32,699] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-2175-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '2175', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 20:31:00,185] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:31:01,081] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:31:37,584] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:31:37,585] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbM119 --master_addr=127.0.0.1 --master_port=29803 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-4199-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 4199 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:31:42,499] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:31:42,499] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29800 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-4182-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 4182 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:31:45,637] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:31:45,659] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:31:46,312] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:31:46,312] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:31:46,313] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:31:46,313] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:31:46,313] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:31:46,313] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:31:46,313] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:31:46,313] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3]}
[2024-05-29 20:31:46,313] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:31:46,313] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:31:46,313] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:31:46,313] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3
[2024-05-29 20:31:46,314] [INFO] [launch.py:253:main] process 27743 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4199-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4199', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:31:46,333] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:31:46,333] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:31:46,333] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:31:46,333] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:31:46,333] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:31:46,334] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:31:46,334] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:31:46,334] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-05-29 20:31:46,334] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:31:46,334] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:31:46,334] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:31:46,334] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-05-29 20:31:46,334] [INFO] [launch.py:253:main] process 27744 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4182-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4182', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:31:53,418] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:31:53,422] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:31:53,603] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:31:53,603] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:31:53,609] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:31:53,609] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:38:10,176] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 26349
[2024-05-29 20:38:10,183] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-8975-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '8975', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-6290-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 6290     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: starting process on GPU 2
-------using deepspeed---------Command isdeepspeed --master_port 29802 --include localhost:2 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-6290-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 6290     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7056-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7056     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: starting process on GPU 2
-------using deepspeed---------Command isdeepspeed --master_port 29802 --include localhost:2 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7056-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7056     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-8219-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 8219     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: starting process on GPU 2
-------using deepspeed---------Command isdeepspeed --master_port 29802 --include localhost:2 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-8219-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 8219     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-8975-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 8975     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: starting process on GPU 2
-------using deepspeed---------Command isdeepspeed --master_port 29802 --include localhost:2 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-8975-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 8975     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19740: finished
[2024-05-29 20:38:33,245] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-29 20:38:33,246] [INFO] [utils.py:801:see_memory_usage] MA 25.72 GB         Max_MA 25.72 GB         CA 25.73 GB         Max_CA 26 GB 
[2024-05-29 20:38:33,247] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 129.19 GB, percent = 14.4%
[2024-05-29 20:38:49,037] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:38:49,869] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:38:49,869] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMl19 --master_addr=127.0.0.1 --master_port=29802 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-1331-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 1331 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
Total number of parameters 6738423808
[2024-05-29 20:38:52,742] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:38:52,970] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Total number of parameters 6738423808
[2024-05-29 20:38:53,670] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:38:53,922] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:38:53,922] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [2]}
[2024-05-29 20:38:53,922] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:38:53,922] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:38:53,922] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:38:53,922] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=2
[2024-05-29 20:38:53,923] [INFO] [launch.py:253:main] process 28558 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-1331-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '1331', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:38:58,890] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 26126
[2024-05-29 20:38:58,891] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7356-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7356', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 20:38:59,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:39:00,177] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-3639-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 3639     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: starting process on GPU 1
-------using deepspeed---------Command isdeepspeed --master_port 29801 --include localhost:1 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-3639-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 3639     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4864-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4864     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: starting process on GPU 1
-------using deepspeed---------Command isdeepspeed --master_port 29801 --include localhost:1 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4864-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4864     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-2689-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 2689     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: starting process on GPU 1
-------using deepspeed---------Command isdeepspeed --master_port 29801 --include localhost:1 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-2689-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 2689     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7356-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7356     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: starting process on GPU 1
-------using deepspeed---------Command isdeepspeed --master_port 29801 --include localhost:1 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7356-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7356     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19739: finished
[2024-05-29 20:39:01,588] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:39:01,775] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:39:01,775] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:39:05,378] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 9.117011547088623 seconds
Time to load cpu_adam op: 9.420387744903564 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:39:19,380] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:39:19,380] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:39:19,679] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:39:19,679] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:39:19,680] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:39:19,684] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:39:19,684] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:39:19,684] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:39:19,684] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:39:21,686] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:39:21,686] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:39:21,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:39:21,882] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:39:21,883] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:39:21,883] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:39:21,883] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:39:21,883] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:39:21,883] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:39:23,080] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:39:23,080] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMV19 --master_addr=127.0.0.1 --master_port=29801 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-2796-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 2796 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:40:18,886] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:40:36,089] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:40:36,089] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [1]}
[2024-05-29 20:40:36,089] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:40:36,090] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:40:36,090] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:40:36,090] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=1
[2024-05-29 20:40:36,180] [INFO] [launch.py:253:main] process 29107 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-2796-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '2796', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
[2024-05-29 20:42:28,197] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:42:28,383] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:42:28,383] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Total number of parameters 6738423808
[2024-05-29 20:45:19,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:47:49,581] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:48:33,582] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 27744
[2024-05-29 20:48:33,583] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4182-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4182', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 15.996112823486328 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:48:48,779] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:48:48,779] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:48:48,978] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:48:48,978] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:48:48,978] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:48:48,978] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:48:48,978] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:48:48,978] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:48:48,978] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7004-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7004     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: starting process on GPU 0
-------using deepspeed---------Command isdeepspeed --master_port 29800 --include localhost:0 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7004-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7004     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7632-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7632     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: starting process on GPU 0
-------using deepspeed---------Command isdeepspeed --master_port 29800 --include localhost:0 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-7632-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 7632     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-2175-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 2175     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: starting process on GPU 0
-------using deepspeed---------Command isdeepspeed --master_port 29800 --include localhost:0 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-2175-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 2175     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: finished
initial command is:deepspeed scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4182-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4182     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: starting process on GPU 0
-------using deepspeed---------Command isdeepspeed --master_port 29800 --include localhost:0 scripts/input_to_label_and_rationale.py                      --output_dir checkpoints_full/esnli-4182-t5-base-1-300-2-0-3e-05-4-300-350-"because"-t5-basestandard-16  --model_type t5-base                       --tokenizer_name t5-base   --task_name esnli  --version v1.0 --do_train --dev_predict                       --logging_first_step  --logging_steps 1  --save_total_limit 1  --seed 4182     --num_train_epochs 2                         --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4                      --early_stopping_patience 1                       --n_shots 16  --fewshot_eval_size 350                       --learning_rate 3e-05  --warmup_steps 0                      --io_format standard  --explanation_sep " because "                      --max_steps 300  --lr_scheduler_type constant  --eval_steps 300 --deepspeed deepspeed_config.json
19738: finished
[2024-05-29 20:49:31,324] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:49:32,527] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:49:32,528] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29800 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-6341-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 6341 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:49:35,172] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:49:35,733] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:49:35,733] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}
[2024-05-29 20:49:35,733] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:49:35,733] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:49:35,733] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:49:35,733] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0
[2024-05-29 20:49:35,734] [INFO] [launch.py:253:main] process 29626 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-6341-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '6341', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
Total number of parameters 6738423808
[2024-05-29 20:49:38,772] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:49:42,197] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:49:42,383] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:49:42,383] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:50:13,585] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
Installed CUDA version 12.0 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
ninja: no work to do.
Time to load cpu_adam op: 19.19511365890503 seconds
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1
[2024-05-29 20:51:23,782] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer
[2024-05-29 20:51:23,782] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-05-29 20:51:23,883] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-05-29 20:51:23,883] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-05-29 20:51:23,883] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 2 optimizer
[2024-05-29 20:51:23,883] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2024-05-29 20:51:23,883] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2024-05-29 20:51:23,883] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-05-29 20:51:23,883] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2024-05-29 20:52:33,078] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-05-29 20:52:33,079] [INFO] [utils.py:801:see_memory_usage] MA 25.72 GB         Max_MA 25.72 GB         CA 25.73 GB         Max_CA 26 GB 
[2024-05-29 20:52:33,176] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 137.32 GB, percent = 15.3%
[2024-05-29 20:53:34,179] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 27743
[2024-05-29 20:53:34,377] [ERROR] [launch.py:322:sigkill_handler] ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-4199-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '4199', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json'] exits with return code = -9
[2024-05-29 20:54:56,380] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:55:13,178] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2024-05-29 20:55:13,178] [INFO] [runner.py:568:main] cmd = /root/miniconda3/envs/feb/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbM119 --master_addr=127.0.0.1 --master_port=29803 --enable_each_rank_log=None scripts/input_to_label_and_rationale.py --output_dir checkpoints_full/esnli-7009-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16 --model_type t5-base --tokenizer_name t5-base --task_name esnli --version v1.0 --do_train --dev_predict --logging_first_step --logging_steps 1 --save_total_limit 1 --seed 7009 --num_train_epochs 2 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 4 --early_stopping_patience 1 --n_shots 16 --fewshot_eval_size 350 --learning_rate 3e-05 --warmup_steps 0 --io_format standard --explanation_sep  because  --max_steps 300 --lr_scheduler_type constant --eval_steps 300 --deepspeed deepspeed_config.json
[2024-05-29 20:55:17,205] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:55:17,803] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:55:17,803] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.5-1
[2024-05-29 20:55:17,803] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-05-29 20:55:17,803] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-05-29 20:55:17,804] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.5-1+cuda12.0
[2024-05-29 20:55:17,804] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.5-1+cuda12.0
[2024-05-29 20:55:17,804] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.5-1
[2024-05-29 20:55:17,804] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [3]}
[2024-05-29 20:55:17,804] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0
[2024-05-29 20:55:17,804] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})
[2024-05-29 20:55:17,804] [INFO] [launch.py:163:main] dist_world_size=1
[2024-05-29 20:55:17,804] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=3
[2024-05-29 20:55:17,805] [INFO] [launch.py:253:main] process 30145 spawned with command: ['/root/miniconda3/envs/feb/bin/python', '-u', 'scripts/input_to_label_and_rationale.py', '--local_rank=0', '--output_dir', 'checkpoints_full/esnli-7009-t5-base-1-300-2-0-3e-05-4-300-350-because-t5-basestandard-16', '--model_type', 't5-base', '--tokenizer_name', 't5-base', '--task_name', 'esnli', '--version', 'v1.0', '--do_train', '--dev_predict', '--logging_first_step', '--logging_steps', '1', '--save_total_limit', '1', '--seed', '7009', '--num_train_epochs', '2', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '4', '--early_stopping_patience', '1', '--n_shots', '16', '--fewshot_eval_size', '350', '--learning_rate', '3e-05', '--warmup_steps', '0', '--io_format', 'standard', '--explanation_sep', ' because ', '--max_steps', '300', '--lr_scheduler_type', 'constant', '--eval_steps', '300', '--deepspeed', 'deepspeed_config.json']
Total number of parameters 6738423808
[2024-05-29 20:55:58,877] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.13.4, git-hash=unknown, git-branch=unknown
[2024-05-29 20:56:57,291] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-05-29 20:56:57,478] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-05-29 20:56:57,478] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-05-29 20:57:15,187] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-05-29 20:57:46,347] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 30145
[2024-05-29 20:57:46,347] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 28558
[2024-05-29 20:57:46,376] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 29626
[2024-05-29 20:57:46,376] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 29107
[2024-05-29 20:57:46,603] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 30145
[2024-05-29 20:57:46,603] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 29107
[2024-05-29 20:57:46,679] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 29626
[2024-05-29 20:57:46,677] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 28558
[2024-05-29 20:58:16,696] [INFO] [launch.py:325:sigkill_handler] Main process received SIGINT, exiting
[2024-05-29 20:58:16,711] [INFO] [launch.py:325:sigkill_handler] Main process received SIGINT, exiting
[2024-05-29 20:58:16,713] [INFO] [launch.py:325:sigkill_handler] Main process received SIGINT, exiting
[2024-05-29 20:58:16,713] [INFO] [launch.py:325:sigkill_handler] Main process received SIGINT, exiting
